{"pages":[{"title":"about","text":"创建这个博客的目的是为了让自己能有个地方能瞎写点东西，记录一下所发生的事情，也没什么他想法，心情好了就更新（逃","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"}],"posts":[{"title":"决策树到底是棵什么树？(1)","text":"简要这篇文章主要讨论3个决策树算法，以及他们的缺点，知道了缺点才能知道改进的方向。 ID3 C4.5 CART ID3描述 ID3 算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。 决策树生成过程 若 D 中所有实例属于同一类 Ck ,则T 为单结点树，并将类 Ck 作为该结点的类标记， 返回 T ； 若 ，则 T 为单结点树，并将 D 中实例数最大的类 Ck 作为该结点的类标记，返回 T； 否则， 计算 A 中各特征对 D 的信息增益，选择信息增益最大的特征 Ag ; 如果 Ag 的信息增益小于阈值，则置 T 为单结点树，并将 D 中实例数最大的类 Ck 作为该结点的类标记，返回 T； 否则，对 Ag 的每一可能值 ai ，依 Ag = ai 将 D 分割为若干非空子集 Di ,将 Di 中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树 T ，返回 T； 对第 i 个子结点，以 Di 为训练集，以 A - {Ag} 为特征集，递归调用1-5步，得到子树 Ti ，返回 Ti； 缺点 算法会增长树的每一个分支的深度，直到恰好能对训练样例完美地分类，所以很容易存在过拟合的问题。 在搜索过程中不进行回溯。所以，它易受无回溯的爬山搜索中的常见风险影响：收敛到局部最优而不是全局最优。 只能处理离散数据，并且倾向于选择取值较多的特征（这是在上篇文章结尾所提到的一个问题，如果只是以信息增益来作为分割特征的选择标准，则会出现这样的问题，我们需要对选择标准做一个修正，这就是接下来要提到的C4.5算法）。 C4.5描述 C4.5算法是在ID3算法的基础上，将选择特征的标志由信息增益换成了信息增益比，其他的生成过程都是一样的。 缺点 在上篇文章结尾我们也提到过，如果只是以信息增益比来作为选择特征的标志，会出现倾向于选择数据量较少的特征。所以更好的改进方法应该是结合信息增益以及信息增益比两者来作为特征选择的标志。 在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。此外，C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。 CART描述 CART算法生成决策树就是递归地构建二叉决策树的过程，对于分类树用基尼指数最小化准则，进行特征选择，生成二叉树。 基尼指数定义分类问题中，假设有 K 个类，样本点属于第 k 类的概率为 pk ，则概率分布的基尼指数定义为如果样本集合 D 根据特征 A 是否取某一可能值 a 被分割成 D1 和 D2 两部分，即则在特征 A 的条件下，集合 D 的基尼指数定义。 数学含义 基尼指数表示的是集合 D 的不确定性，基尼指数越大，样本集合的不确定性也就越大，这一点与熵相似。Gini(p) 与 0.5H(p) 对于二分类问题时的函数图像较为接近。 决策树生成过程 设结点的训练集为 D ，计算现有特征对该数据集的基尼指数。此时，对每一个特征 A，对其可能取的每个值 a，根据样本点对 A=a 的预测为“是”或“否”将 D 分割成 D1 和 D2 两部分，计算 A=a 时的基尼指数。 在所有可能的特征 A 以及它们所有可能的切分点 a 中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。 对两个子结点递归地调用1、2，直到满足停止条件。算法停止计算的条件是结点中的样本个数小于预定阈值，或样本集的基尼指数小于预定阈值，或者没有更多特征。 缺点 CART相对于前两种算法来说没有明显的独有缺点，但还是有决策树的一些通病:容易过拟合。 决策树过拟合的解决方法，要么是通过阈值控制终止条件避免树形结构分支过细，要么就是通过对已经形成的决策树进行剪枝来避免过拟合。所以阈值的设定其实也是一个比较重要的点。 还有一个问题就是，如果样本发生一点点的改动，就会导致树结构的剧烈改变。当然现在是有算法可以解决这个问题的，今后也会提到这个算法。 在做特征选择的时候都是选择最优的一个特征来做分类决策，但是大多数，分类决策不应该是由某一个特征决定的，而是应该由一组特征决定的。这也是一个思考的方向。 Reference:[1] 《统计学习方法》","link":"/2018/12/11/jueceshu2/"},{"title":"决策树到底是棵什么树？(2)","text":"简要前面我们提到了决策树的特征选择以及决策树的生成，那么想要让一棵树变得更好看，修剪则是有必要的，所以这篇文章会讲讲决策树算法三大步骤的最后一步——剪枝。 剪枝 前面我们讨论过决策树学习的模型的各种缺点，过拟合几乎是一个通病，过拟合的原因就在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出了过于复杂的决策树。 那么在决策树学习中将已生成的树进行简化的过程称为剪枝，具体的就是从已生成的树上裁掉一些子树或叶结点，并将其根结点或父节点作为新的叶结点，从而简化分类模型。 决策树学习的剪枝算法决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。 定义设树T 的叶结点个数为| T |，t 是树 T 的叶结点，该叶结点有 Nt 个样本点，其中 k 类的样本点有 Ntk 个，k=1,2,…,K, Ht(T) 为叶结点 t 上的经验熵， a&gt;=0 为参数，则决策树学习的损失函数可以定义为其中经验熵为将损失函数右端第一项记作这时有 数学含义 C(T) 表示模型对训练数据的预测误差（不同的决策树生成算法他的预测误差的公式不同，例如CART利用的是基尼指数，C4.5利用的是信息增益比） |T| 表示模型复杂度，参数α则是控制模型复杂度和预测误差之间的影响 剪枝过程 计算每个结点的经验熵 递归地从树的结点向上回缩，比较回缩到父结点之前与之后的整体树的损失函数，如果之前比之后的大，则进行剪枝，将父结点变成新的叶结点。 返回2，直到不能继续为止，得到损失函数最小的子树Tα 上面这个过程是一个较为简略的描述，针对不同的算法，剪枝的过程也各有一些细节上的不同。 Reference:[1] 《统计学习方法》","link":"/2018/12/13/jueceshu3/"},{"title":"最后还是搭了博客。。。。。。真香","text":"缘由 我本来就是一个不怎么喜欢发动态的人，这就更别说写博客了，所以按我以前的性格来说是不会去搭博客的。但是为了锻炼一下自己的写作能力还有就是最近开始想记录一些生活中的事情和学习的笔记以及想开始做一些算是成品的东西，所以最后还是搭了一个（真香 总的来说搭起来还是蛮简单的，其实我在这次之前也尝试着搭过一次，第一次是想用jekyll来搭的（看着jekyll主题还不错，结果发现贼难弄，需要安装各种东西，特别还记得需要安装Ruby，emmmm，接着就是各种报错，虽然最后安装上了，但是部署网页又总是不成功，最后就放弃了。 今天换了Hexo感觉简单多了好吧，主题也是差不多的，其实第一次搭的时候我也知道Hexo，但没怎么在意，emmmm，这可能也是今后需要改进的一个点吧 接下来的打算就是写一些关于自己平时所做的一些事情的文章，即是一种对我自己生活的记录，也是相当于做一个笔记，以后可以回过头来复习看看。 以上","link":"/2018/12/03/my-test-file/"},{"title":"决策树到底是棵什么树？(0)","text":"简要因为最近在看决策树的知识，所以想做个总结。应该会分几篇来总结，主要讨论的是3个算法,讨论算法之前先来理一理数学公式。 什么是决策树？ 决策树它既不是铁树，也不是松树，更不是核桃树。我们可以拆成两部分来看 决策以及树。决策是它的功能，而树则是他的结构。功能与结构相结合便成为了模型。决策树的功能也分为分类与回归，我们会讨论的是分类的功能。 决策树学习决策树的学习主要有三个步骤 特征选择 决策树的生成 决策树的剪枝 所以对于前面提到的三种算法的讨论也会按着这3大步骤去进行叙述。 信息增益 在开始讨论算法之前，先对所需要的数学知识做一个简单的梳理，方便后面更好去理解模型对于数学知识的应用所产生的效果，首先的一个是信息增益，但在说信息增益之前先要说明熵与条件熵 熵定义： 表示随机变量不确定性的度量设 X 是一个取有限个值的离散随机变量，其概率分布为 则随机变量 X 的熵定义为 数学含义：熵越大，随机变量的不确定性越大（就是说熵越大随机变量越不知道取哪个值，反之熵越小越确定随机变量要取哪个值） 当随机变量只有两个值时，例如0，1时，设取1的概率为p，则H（X）在 p=0.5处会取得最大值（函数图像就省略了，画起来比较麻烦），说明当p=0.5随机变量的不确定度最大，只有百分之50的概率去判断是1，这就无法非常清晰的知道随机变量到底是取1还是取0，所以是存在了一个不确定的因素。 反之如果p=0.8时，H（X）则会变得很小，说明随机变量有80%的概率等于1让公式的不确定性有了一个降低，随机变量更加确切的要取1，这就是对这个公式所表达的一个不确定性的解释。 当随机变量有三个值时，例如0，1，2时，设值为0时的概率为P0，值为1时的概率为P1，这时则需要用3维的坐标对H（X）函数进行表示，x，y轴表示P0与P1的值，z轴则表示H（X）的值。所以随着变量可取值数量的增加维度也会随之上升。 条件熵定义： 随机变量 X 给定的条件下，随机变量 Y 的不确定性设有随机变量（X，Y），其联合概率分布为 条件熵 H（Y | X） 定义为 X 给定条件下， Y 的条件概率分布的熵对X的数学期望 数学含义：Y 在 X 的条件下熵越大，随机变量的不确定性越大（同熵的概念，只是在上的基础上加上了一个特征的限定） 要注意的是条件熵由两部分构成，一部分是条件概率分布的熵，一部分是期望。 条件概率分布的熵计算的则是特征A的某个取值下，随机变量Y取值的不确定性的大小，即是熵的大小。 对X进行期望计算则是因为，在特征A的每个值的条件下，随机变量Y的不确定性（熵）都不相同，为了得到总的来说特征A（而不是某个取值）对于随机变量取值的不确定性的改变，则进行期望计算，得到的是特征A的某个取值下的随机变量的取值不确定性的改变的平均值，这个平均值则是特征A对于随机变量Y的取值，总的不确定性的改变。 信息增益定义： 特征A对训练数据集D的信息增益g(D，A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H（D|A）之差 当 g(D,A) &gt; 0 时，不确定性下降，说明有特征A的限定与没有特征A的限定相比较起来，有特征A的限定能帮助我们更好的去确定随机变量的取值 当 g(D,A) &lt; 0 时，不确定性上升，说明有特征A的限定与没有特征A的限定相比较起来，有特征A的限定不能帮助我们更好的去确定随机变量的取值 我们就根据这样，特征对于随机变量取值的确定性是否有帮助，来去选择我们所需要的特征 信息增益比定义： 特征A对训练数据集D的信息增益比gR(D，A)，定义其信息增益g(D，A)与训练数据集D关于特征A的值的熵HA（D）之比 其中， n是特征A取值的个数。 如果单是用信息增益来判读选取哪个特征来帮助我们确定随机变量的值，会存在偏向于选择在数据集中取值较多的特征的问题（这样就会因为数据集的特殊性，而影响了我们判断真实的情况是怎么样的），根据函数的图像我们可以知道这一点。 信息增益比可以对这个问题进行一个矫正，这是选择特征的另外一种标准 缺点是信息增益比偏向取值较少的特征，原因是当特征取值较少时HA(D)的值较小，因此其倒数较大，因而信息增益比较大。因而偏向取值较少的特征。 基于以上缺点，并不是直接选择信息增益比最大的特征，而是先在候选特征中找出信息增益高于平均水平的特征，然后在这些特征中再选择信息增益比最高的特征。 Reference:[1] 《统计学习方法》","link":"/2018/12/05/jiuceshu/"}],"tags":[{"name":"统计学习方法","slug":"统计学习方法","link":"/tags/统计学习方法/"},{"name":"日常","slug":"日常","link":"/tags/日常/"}],"categories":[{"name":"统计学习方法","slug":"统计学习方法","link":"/categories/统计学习方法/"},{"name":"日常","slug":"日常","link":"/categories/日常/"},{"name":"决策树","slug":"统计学习方法/决策树","link":"/categories/统计学习方法/决策树/"},{"name":"最后还是搭了博客......真香","slug":"日常/最后还是搭了博客-真香","link":"/categories/日常/最后还是搭了博客-真香/"}]}