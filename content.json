{"pages":[{"title":"about","text":"创建这个博客的目的是为了让自己能有个地方能瞎写点东西，记录一下所发生的事情，也没什么他想法，心情好了就更新（逃","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"}],"posts":[{"title":"决策树到底是棵什么树？(0)","text":"简要因为最近在看决策树的知识，所以想做个总结。应该会分几篇来总结，主要讨论的是3个算法,讨论算法之前先理一理数学公式。 什么是决策树？ 决策树它既不是铁树，也不是松树，更不是核桃树。我们可以拆成两部分来看 决策以及树。决策是它的功能，而树则是他的结构。功能与结构相结合便成为了模型。决策树的功能也分为分类与回归，我们会讨论的是分类的功能。 决策树学习决策树的学习主要有三个步骤 特征选择 决策树的生成 决策树的剪枝 所以对于前面提到的三种算法的讨论也会按着这3大步骤去进行叙述。 信息增益 在开始讨论算法之前，先对所需要的数学知识做一个简单的梳理，方便后面更好去理解模型对于数学知识的应用所产生的效果，首先的一个是信息增益，但在说信息增益之前先要说明熵与条件熵 熵定义： 表示随机变量不确定性的度量设 X 是一个取有限个值的离散随机变量，其概率分布为 则随机变量 X 的熵定义为 数学含义：熵越大，随机变量的不确定性越大（就是说熵越大随机变量越不知道取哪个值，反之熵越小越确定随机变量要取哪个值） 当随机变量只有两个值时，例如0，1时，设取1的概率为p，则H（X）在 p=0.5处会取得最大值（函数图像就省略了，画起来比较麻烦），说明当p=0.5随机变量的不确定度最大，只有百分之50的概率去判断是1，这就无法非常清晰的知道随机变量到底是取1还是取0，所以是存在了一个不确定的因素。 反之如果p=0.8时，H（X）则会变得很小，说明随机变量有80%的概率等于1让公式的不确定性有了一个降低，随机变量更加确切的要取1，这就是对这个公式所表达的一个不确定性的解释。 当随机变量有三个值时，例如0，1，2时，设值为0时的概率为P0，值为1时的概率为P1，这时则需要用3维的坐标对H（X）函数进行表示，x，y轴表示P0与P1的值，z轴则表示H（X）的值。所以随着变量可取值数量的增加维度也会随之上升。 条件熵定义： 随机变量 X 给定的条件下，随机变量 Y 的不确定性设有随机变量（X，Y），其联合概率分布为 条件熵 H（Y | X） 定义为 X 给定条件下， Y 的条件概率分布的熵对X的数学期望 数学含义：Y 在 X 的条件下熵越大，随机变量的不确定性越大（同熵的概念，只是在上的基础上加上了一个特征的限定） 要注意的是条件熵由两部分构成，一部分是条件概率分布的熵，一部分是期望。 条件概率分布的熵计算的则是特征A的某个取值下，随机变量Y取值的不确定性的大小，即是熵的大小。 对X进行期望计算则是因为，在特征A的每个值的条件下，随机变量Y的不确定性（熵）都不相同，为了得到总的来说特征A（而不是某个取值）对于随机变量取值的不确定性的改变，则进行期望计算，得到的是特征A的某个取值下的随机变量的取值不确定性的改变的平均值，这个平均值则是特征A对于随机变量Y的取值，总的不确定性的改变。 信息增益定义： 特征A对训练数据集D的信息增益g(D，A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H（D|A）之差 当 g(D,A) &gt; 0 时，不确定性下降，说明有特征A的限定与没有特征A的限定相比较起来，有特征A的限定能帮助我们更好的去确定随机变量的取值 当 g(D,A) &lt; 0 时，不确定性上升，说明有特征A的限定与没有特征A的限定相比较起来，有特征A的限定不能帮助我们更好的去确定随机变量的取值 我们就根据这样，特征对于随机变量取值的确定性是否有帮助，来去选择我们所需要的特征 信息增益比定义： 特征A对训练数据集D的信息增益比gR(D，A)，定义其信息增益g(D，A)与训练数据集D关于特征A的值的熵HA（D）之比 其中， n是特征A取值的个数。 如果单是用信息增益来判读选取哪个特征来帮助我们确定随机变量的值，会存在偏向于选择在数据集中取值较多的特征的问题（这样就会因为数据集的特殊性，而影响了我们判断真实的情况是怎么样的），根据函数的图像我们可以知道这一点。 信息增益比可以对这个问题进行一个矫正，这是选择特征的另外一种标准 缺点是信息增益比偏向取值较少的特征，原因是当特征取值较少时HA(D)的值较小，因此其倒数较大，因而信息增益比较大。因而偏向取值较少的特征。 基于以上缺点，并不是直接选择信息增益比最大的特征，而是先在候选特征中找出信息增益高于平均水平的特征，然后在这些特征中再选择信息增益比最高的特征。","link":"/2018/12/05/jiuceshu/"},{"title":"决策树到底是棵什么树？(1)","text":"简要这篇文章主要讨论3个决策树算法 ID3 C4.5 CART ID3描述 ID3 算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。 决策树生成过程 若 D 中所有实例属于同一类 Ck ,则T 为单结点树，并将类 Ck 作为该结点的类标记， 返回 T ； 若 ，则 T 为单结点树，并将 D 中实例数最大的类 Ck 作为该结点的类标记，返回 T； 否则， 计算 A 中各特征对 D 的信息增益，选择信息增益最大的特征 Ag ; 如果 Ag 的信息增益小于阈值，则置 T 为单结点树，并将 D 中实例数最大的类 Ck 作为该结点的类标记，返回 T； 否则，对 Ag 的每一可能值 ai ，依 Ag = ai 将 D 分割为若干非空子集 Di ,将 Di 中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树 T ，返回 T； 对第 i 个子结点，以 Di 为训练集，以 A - {Ag} 为特征集，递归调用1-5步，得到子树 Ti ，返回 Ti； 缺点 算法增长树的每一个分支的深度，直到恰好能对训练样例完美地分类，存在决策树过度拟合。 在搜索过程中不进行回溯。所以，它易受无回溯的爬山搜索中的常见风险影响：收敛到局部最优而不是全局最优。 只能处理离散数据，并且倾向于选择取值较多的特征（这是在上篇文章结尾所提到的一个问题，如果只是以信息增益来作为分割特征的选择标准，则会出现这样的问题，我们需要对选择标准做一个修正，这就是接下来要提到的C4.5算法）。 C4.5描述","link":"/2018/12/11/jueceshu2/"},{"title":"最后还是搭了博客。。。。。。真香","text":"缘由 我本来就是一个不怎么喜欢发动态的人，这就更别说写博客了，所以按我以前的性格来说是不会去搭博客的。但是为了锻炼一下自己的写作能力还有就是最近开始想记录一些生活中的事情和学习的笔记以及想开始做一些算是成品的东西，所以最后还是搭了一个（真香 总的来说搭起来还是蛮简单的，其实我在这次之前也尝试着搭过一次，第一次是想用jekyll来搭的（看着jekyll主题还不错，结果发现贼难弄，需要安装各种东西，特别还记得需要安装Ruby，emmmm，接着就是各种报错，虽然最后安装上了，但是部署网页又总是不成功，最后就放弃了。 今天换了Hexo感觉简单多了好吧，主题也是差不多的，其实第一次搭的时候我也知道Hexo，但没怎么在意，emmmm，这可能也是今后需要改进的一个点吧 接下来的打算就是写一些关于自己平时所做的一些事情的文章，即是一种对我自己生活的记录，也是相当于做一个笔记，以后可以回过头来复习看看。 以上","link":"/2018/12/03/my-test-file/"}],"tags":[{"name":"统计学习方法","slug":"统计学习方法","link":"/tags/统计学习方法/"},{"name":"日常","slug":"日常","link":"/tags/日常/"}],"categories":[{"name":"统计学习方法","slug":"统计学习方法","link":"/categories/统计学习方法/"},{"name":"日常","slug":"日常","link":"/categories/日常/"},{"name":"决策树","slug":"统计学习方法/决策树","link":"/categories/统计学习方法/决策树/"},{"name":"最后还是搭了博客......真香","slug":"日常/最后还是搭了博客-真香","link":"/categories/日常/最后还是搭了博客-真香/"}]}