{"pages":[{"title":"about","text":"创建这个博客的目的是为了让自己能有个地方能瞎写点东西，记录一下所发生的事情，也没什么他想法，心情好了就更新（逃","link":"/about/index.html"}],"posts":[{"title":"决策树到底是棵什么树？(0)","text":"简要因为最近在看决策树的知识，所以想做个总结。应该会分很多篇来总结，主要讨论的是3个算法 ID3 C4.5 CART 什么是决策树？ 决策树它既不是铁树，也不是松树，更不是核桃树。我们可以拆成两部分来看 决策以及树。决策是它的功能，而树则是他的结构。功能与结构相结合便成为了模型。决策树的功能也分为分类与回归，我们讨论的是分类的功能，不讨论回归 决策树学习决策树的学习主要有三个步骤 特征选择 决策树的生成 决策树的剪枝 所以接下来对于前面提到的三种算法的讨论也会按着这3大步骤去进行 信息增益 在开始讨论算法之前，先对所需要的数学知识做一个简单的梳理，方便后面更好去理解模型对于数学知识的应用所产生的效果，在说信息增益之前先说明熵与条件熵 熵定义： 表示随机变量不确定性的度量设 X 是一个取有限个值的离散随机变量，其概率分布为 则随机变量 X 的熵定义为 数学含义：熵越大，随机变量的不确定性越大 当随机变量只有两个值时，例如0，1时，H（X）在 p=0.5处会取得最大值（函数图像就省略了，画起来比较麻烦），说明当p=0.5随机变量的不确定度最大，只有百分之50的概率去判断是1或者是0，这就无法非常清晰的的对随机变量进行一个分类，所以是存在了一个不确定的因素。 反之如果p=0.8时，H（X）则会变得很小，说明80%的概率让公式的不确定性有了一个降低，对于类别的分类变得更加确切，这就是对这个公式所表达的一个不确定性的解释。 当随机变量有三个值时，例如0，1，2时，设值为0时的概率为P0，值为1时的概率为P1，这时则需要用3维的坐标对H（X）函数进行表示，x，y轴表示P0与P1的值，z轴则表示H（X）的值。所以随着变量可取值数量的增加维度也会随之上升。 条件熵定义： 随机变量 X 给定的条件下，随机变量 Y 的不确定性设有随机变量（X，Y），其联合概率分布为 条件熵 H（Y | X） 定义为 X 给定条件下， Y 的条件概率分布的熵对X的数学期望 数学含义：Y 在 X 的条件下熵越大，随机变量的不确定性越大 信息增益定义： 特征A对训练数据集D的信息增益g(D，A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H（D|A）之差","link":"/2018/12/05/jiuceshu/"},{"title":"最后还是搭了博客。。。。。。。。真香","text":"缘由 我本来就是一个不怎么喜欢发动态的人，这就更别说写博客了，所以按我以前的性格来说是不会去搭博客的。但是为了锻炼一下自己的写作能力还有就是最近开始想记录一些生活中的事情和学习的笔记以及想开始做一些算是成品的东西，所以最后还是搭了一个（真香 总的来说搭起来还是蛮简单的，其实我在这次之前也尝试着搭过一次，第一次是想用jekyll来搭的（看着jekyll主题还不错，结果发现贼难弄，需要安装各种东西，特别还记得需要安装Ruby，emmmm，接着就是各种报错，虽然最后安装上了，但是部署网页又总是不成功，最后就放弃了。 今天换了Hexo感觉简单多了好吧，主题也是差不多的，其实第一次搭的时候我也知道Hexo，但没怎么在意，emmmm，这可能也是今后需要改进的一个点吧 接下来的打算就是写一些关于自己平时所做的一些事情的文章，即是一种对我自己生活的记录，也是相当于做一个笔记，以后可以回过头来复习看看。 以上","link":"/2018/12/03/my-test-file/"}],"tags":[{"name":"统计学习方法","slug":"统计学习方法","link":"/tags/统计学习方法/"},{"name":"日常","slug":"日常","link":"/tags/日常/"}],"categories":[]}